{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceeaf29d",
   "metadata": {},
   "source": [
    "## Training SCSS-Net on active regions\n",
    "In this notebook, we provide script to train SCSS-Net to segment active regions on 171A images.   \n",
    "Training process is shown at the bottom   \n",
    "More on evaluating SCSS-net output in evaulating_SCSS-net.ipynb file and evaulating96-21_SCSS-net.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4599b1bb-4592-4a74-973b-653e9ad74456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fc2cff-a894-4564-8f01-acb989a809a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64dd56e-19ab-440b-87db-86c7b7e4262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_scss_net import scss_net\n",
    "from metrics import dice_np, iou_np, dice, iou\n",
    "from utils import plot_imgs, plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cad209e-1428-486c-b9ec-07d0c4915b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "IMG_SIZE = 256  # resize imgs to 256x256\n",
    "BATCH_SIZE = 20 # set batch size\n",
    "SEED = 42       # set seed for reproducibility\n",
    "EPOCHS = 100    # Set number of epochs\n",
    "\n",
    "# specify model filename, you should rewrite MODEL_NAME as yours preferrings\n",
    "MODEL_NAME = \"model_filename\"\n",
    "model_filename = f\"{MODEL_NAME}.h5\"                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc22eb13-be83-4439-88f2-5498a8d08e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imgs number = 2621\n",
      "Masks number = 2621\n"
     ]
    }
   ],
   "source": [
    "imgs = glob.glob(\"../data/train_test_data/AR_train_imgs/*.png\")\n",
    "masks = glob.glob(\"../data/train_test_data/AR_train_masks/*.png\")\n",
    "\n",
    "print(f\"Imgs number = {len(imgs)}\\nMasks number = {len(masks)}\")\n",
    "\n",
    "imgs_list = []\n",
    "masks_list = []\n",
    "for image, mask in zip(imgs, masks):\n",
    "    imgs_list.append(np.array(Image.open(image).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))))\n",
    "    masks_list.append(np.array(Image.open(mask).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4b69ed-4905-44ad-b1cc-b5d64d3f4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization from (0; 255) to (0; 1)\n",
    "x = np.asarray(imgs_list, dtype=np.float32)/255\n",
    "y = np.asarray(masks_list, dtype=np.float32)/255\n",
    "\n",
    "# Reshape to (n_imgs, height, width, channels)\n",
    "x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "y = y.reshape(y.shape[0], y.shape[1], y.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0ab8b8-bb33-4482-9b92-53f3691ee7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d43094-14a1-47a1-bcc9-007d7780b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (256, 256, 1)\n",
      "Train shape: (2096, 256, 256, 1)  Val shape: (525, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# input shape should be (256, 256, 1)\n",
    "input_shape = x_train[0].shape\n",
    "print(f\"Input shape: {input_shape}\\nTrain shape: {x_train.shape}  Val shape: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15a721b-b89b-4ccb-ae37-c1aeb419c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model architecture with optimal parameteres\n",
    "model = scss_net( \n",
    "    input_shape,\n",
    "    filters=32,       \n",
    "    layers=4,\n",
    "    batch_norm=True,\n",
    "    drop_prob=0.5)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",  \n",
    "    metrics=[iou, dice])\n",
    "\n",
    "# Set steps parameters acording to size of training set and size of batch\n",
    "STEPS = x_train.shape[0] // BATCH_SIZE        \n",
    "\n",
    "# Set Callback that saves only best weights\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    model_filename,\n",
    "    verbose=1,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf4fddc-1a6d-497f-ac13-24296ca3fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, uncoment following lines to train\n",
    "# history = model.fit(\n",
    "#    x_train,\n",
    "#    y_train,\n",
    "#    steps_per_epoch=STEPS,\n",
    "#    epochs=EPOCHS,\n",
    "#    validation_data=(x_val, y_val),\n",
    "#    callbacks=[callback_checkpoint],\n",
    "#    verbose=2)\n",
    "\n",
    "# # Plot training history (Metrics and Loss)\n",
    "# plot_metrics(history).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db2cb05-20f6-4912-9ced-92bec3f223f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"ar_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c4935a2",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "<img src=\"../figures/training_processAR.png\"  width=\"1200\" height=\"120\"> \n",
    "\n",
    "<img src=\"../figures/training_graphAR.png\"  width=\"908\" height=\"280\">   \n",
    "\n",
    "training with 2095 images took approx. 22 hours on Macbook pro M1 whitout GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f249c492",
   "metadata": {},
   "source": [
    "Sample of segmented active regions  \n",
    "\n",
    "<img src=\"../figures/AR-predicted.png\"  width=\"1800\" height=\"8000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7905f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
